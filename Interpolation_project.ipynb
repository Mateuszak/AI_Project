{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JaGCBAIORzfB"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VXIcbmKOhjOo"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "import collections\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.transforms\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enable = True\n",
    "\n",
    "generate = True\n",
    "frameFolderPath = './video_frames/'\n",
    "\n",
    "videos = os.listdir('drive/My Drive/training_videos')\n",
    "\n",
    "if generate:\n",
    "    count = 0\n",
    "    for video in videos:\n",
    "        vidcap = cv2.VideoCapture('drive/My Drive/training_videos/' + video)\n",
    "        success,image = vidcap.read()\n",
    "        print(success)\n",
    "        while success:\n",
    "            cv2.imwrite(frameFolderPath + \"frame%d.png\" % count, image)      \n",
    "            success,image = vidcap.read()\n",
    "            count += 1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A31NVbvsNVap"
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "vidcap = cv2.VideoCapture('drive/My Drive/training_videos/all_test11346.avi')\n",
    "success,image = vidcap.read()\n",
    "print(success)\n",
    "while success:\n",
    "    cv2.imwrite('test_frames/frame%d.png' % count, image)      \n",
    "    success,image = vidcap.read()\n",
    "    count += 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gcpFflkhhjOs"
   },
   "outputs": [],
   "source": [
    "class FrameLoader(Dataset):\n",
    "    def __init__(self, root_dir, transform = None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.frame_list = os.listdir(root_dir)\n",
    "        self.frame_list.sort(key = lambda x: os.stat(root_dir + x).st_ctime)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frame_list) // 3 - 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        frame1 = Image.open(self.root_dir + self.frame_list[3 * idx])\n",
    "        frame2 = Image.open(self.root_dir + self.frame_list[3 * idx + 2])\n",
    "        intermediate = Image.open(self.root_dir + self.frame_list[3 * idx + 1])\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            frame1 = self.transform(frame1)\n",
    "            frame2 = self.transform(frame2)\n",
    "            intermediate = TF.resize(intermediate, (256, 448))\n",
    "            intermediate = TF.to_tensor(intermediate)\n",
    "        else:\n",
    "            frame1 = TF.to_tensor(frame1)\n",
    "            frame2 = TF.to_tensor(frame2)\n",
    "            intermediate = TF.to_tensor(intermediate)\n",
    "        \n",
    "            \n",
    "        return [torch.cat((frame1, frame2)), intermediate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mHKa6OUlhjOv"
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "dataset = FrameLoader('./video_frames/')\n",
    "testset = FrameLoader('./test_frames/')\n",
    "data_loader = DataLoader(dataset, batch_size = batch_size, num_workers = 4, shuffle = True, pin_memory = True)\n",
    "test_loader = DataLoader(testset, batch_size = batch_size, num_workers = 4, shuffle = True, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qYsx_A5PhjOz"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def convolution_pack(channels_in, channels_out, kernel_size, padding_size):\n",
    "    return nn.Sequential(nn.Conv2d(channels_in, channels_out, kernel_size, padding = padding_size),\n",
    "                       nn.ReLU(),\n",
    "                       nn.Conv2d(channels_out, channels_out, kernel_size, padding = padding_size),\n",
    "                       nn.ReLU(),\n",
    "                       nn.Conv2d(channels_out, channels_out, kernel_size, padding = padding_size),\n",
    "                       nn.ReLU(),\n",
    "                       nn.Conv2d(channels_out, channels_out, kernel_size, padding = padding_size),\n",
    "                       nn.ReLU(),\n",
    "                       nn.Conv2d(channels_out, channels_out, 2, stride = 2),\n",
    "                       nn.ReLU()\n",
    "                       )\n",
    "\n",
    "def transposed_convolution(channels_in, channels_out, kernel_size, padding_size):\n",
    "    return nn.Sequential(nn.Upsample(scale_factor=2, mode = 'nearest'),\n",
    "                       nn.ReLU(),\n",
    "                       nn.Conv2d(channels_in, channels_out, kernel_size, padding = padding_size),\n",
    "                       nn.ReLU(),\n",
    "                       nn.Conv2d(channels_out, channels_out, kernel_size, padding = padding_size),\n",
    "                       nn.ReLU(),\n",
    "                       nn.Conv2d(channels_out, channels_out, kernel_size, padding = padding_size),\n",
    "                       nn.ReLU(),\n",
    "                       nn.Conv2d(channels_out, channels_out, kernel_size, padding = padding_size),\n",
    "                       nn.ReLU()\n",
    "                       )\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.pack1 = convolution_pack(6, 32, 3, 1)\n",
    "        self.pack2 = convolution_pack(32, 64, 3, 1)\n",
    "        self.pack3 = convolution_pack(64, 128, 3, 1)\n",
    "        self.pack4 = convolution_pack(128, 256, 3, 1)\n",
    "        self.pack5 = convolution_pack(256, 512, 3, 1)\n",
    "        self.transpose2 = transposed_convolution(512, 256, 3, 1)\n",
    "        self.transpose3 = transposed_convolution(256, 128, 3, 1)\n",
    "        self.transpose4 = transposed_convolution(128, 64, 3, 1)\n",
    "        self.transpose5 = transposed_convolution(64, 32, 3, 1)\n",
    "        self.transpose6 = transposed_convolution(32, 3, 3, 1)\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pack1 = self.pack1(x)\n",
    "        pack2 = self.pack2(pack1)\n",
    "        pack3 = self.pack3(pack2)\n",
    "        pack4 = self.pack4(pack3)\n",
    "        pack5 = self.pack5(pack4)\n",
    "        x = self.transpose2(pack5)\n",
    "        x = self.transpose3(x + pack4)\n",
    "        x = self.transpose4(x + pack3)\n",
    "        x = self.transpose5(x + pack2)\n",
    "        x = self.transpose6(x + pack1)\n",
    "            \n",
    "        return x\n",
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p7zBp062k0CH"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from pytorch_msssim import MS_SSIM\n",
    "\n",
    "ms_ssim_module = MS_SSIM(data_range=255, size_average=True, channel=3)\n",
    "\n",
    "optimizer = optim.Adamax(net.parameters(), lr = 1e-4)\n",
    "loss_function = nn.L1Loss()\n",
    "\n",
    "epochs = 10\n",
    "alpha = 0.84  \n",
    "\n",
    "training_loss = []\n",
    "\n",
    "for epoch in range(epochs): \n",
    "    for inputs, labels in tqdm(data_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device) \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = alpha * (1 - ms_ssim_module(outputs, labels)) + (1 - alpha) * loss_function(outputs, labels)\n",
    "        training_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch: {} Loss: {}\".format(epoch + 1, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O3IWKx7qcWuM"
   },
   "outputs": [],
   "source": [
    "from SSIM_PIL import compare_ssim\n",
    "from tqdm import tqdm\n",
    "ssim_sum = 0\n",
    "with torch.no_grad():\n",
    "    for sample, target in tqdm(test_loader):\n",
    "    output = net(sample.to(device))\n",
    "    for i, frame in enumerate(output):\n",
    "        image1 = torchvision.transforms.ToPILImage()(frame.cpu())\n",
    "        image2 = torchvision.transforms.ToPILImage()(target[i].cpu())\n",
    "        ssim_sum += compare_ssim(image1, image2)\n",
    "    print(\"Average ssim score: {}\".format(ssim_sum / (batch_size * len(test_loader))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W_btSG88QNe4"
   },
   "outputs": [],
   "source": [
    "import numpy \n",
    "import math\n",
    "PIXEL_MAX = 255.0\n",
    "def psnr(img1, img2):\n",
    "    img1 = numpy.array(img1)\n",
    "    img2 = numpy.array(img1)\n",
    "    mse = numpy.mean( (img1 - img2) ** 2 )\n",
    "    if mse == 0:\n",
    "        return 100\n",
    "    return 20 * math.log10(PIXEL_MAX) - 10 * math.log10(mse)\n",
    "\n",
    "psnr_sum = 0\n",
    "with torch.no_grad():\n",
    "    for sample, target in tqdm(test_loader):\n",
    "        output = net(sample.to(device))\n",
    "        for i, frame in enumerate(output):\n",
    "            image1 = torchvision.transforms.ToPILImage()(frame.cpu())\n",
    "            image2 = torchvision.transforms.ToPILImage()(target[i].cpu())\n",
    "            psnr_sum += psnr(image1, image2)\n",
    "print(\"Average psnr score: {}\".format(psnr_sum / (batch_size * len(test_loader))))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DataPreprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
